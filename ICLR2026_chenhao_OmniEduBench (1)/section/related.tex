\section{Related Work}

In this section, we present a comprehensive survey of large language models (LLMs) and benchmarks related to our constructed OmniEduBench, encompassing both English and Chinese datasets.

\subsection{Large Language Models}

Recently large language models have advanced at an unprecedented pace. Leveraging increasingly sophisticated architectures and ever-larger pretraining corpora, they have continuously pushed the boundaries of performance in language understanding, reasoning, and generation tasks. Researchers have explored various approaches to enhance LLMs’ capabilities. For example, Chain-of-Thought prompting~\citep{wei2022chain,qwq32b,seed2025seed-oss,guo2025deepseek,liu2024deepseek-v2} has been shown to be highly effective in guiding models to perform step-by-step reasoning for complex problem-solving. In addition, instruction tuning~\citep{dongerict,huadvancing,qwen2.5,yang2025qwen3} and reinforcement learning from human feedback (RLHF)~\citep{ouyang2022training,schulman2017proximal} have been widely adopted to align model outputs with human intentions and preferences, enabling LLMs to generate responses that are more natural and reliable in open-ended dialogue and creative tasks. Despite these remarkable advances, however, the question of how to comprehensively and effectively evaluate the true capabilities of LLMs remains a critical and open challenge.

\subsection{English Education Benchmarks}

Researchers have proposed a variety of benchmarks to evaluate the capabilities of LLMs, which can be broadly categorized into three types: (1) task-specific evaluations, such as reading comprehension (SQuAD~\citep{rajpurkar2016squad}), machine translation~\citep{bojar-etal-2014-findings}), and summarization~\citep{hermann2015teaching}; (2) general knowledge and advanced ability evaluations, for example, the Massive Multitask Language Understanding (MMLU) benchmark~\citep{hendrycks2021measuring}, which collects questions from real-world exams and textbooks to provide a diverse, multi-domain test that effectively probes the breadth and depth of model knowledge. Similarly, the BIG-bench benchmark~\citep{srivastava2022beyond} comprises 204 diverse tasks; and (3) specialized ability evaluations. In mathematical reasoning, benchmarks such as GSM8K~\citep{cobbe2021gsm8k} and MATH~\citep{hendrycks2021measuring} assess models’ ability to solve complex multi-step problems. In code generation, HumanEval~\citep{chen2021evaluating} and MBPP~\citep{austin2021program} have become standard benchmarks for measuring programming proficiency. Additionally, datasets such as MT-bench~\citep{zheng2023judging} have been introduced to evaluate performance in multi-turn, open-ended dialogues. Despite the significant contributions of these datasets to advancing LLMs evaluation, most of them remain heavily focused on English, with limited coverage of Chinese scenarios.

\subsection{Chinese Education Benchmarks}

A series of comprehensive Chinese benchmarks have been proposed. For example, CLUE~\citep{xu2020clue}, as an early work, integrates multiple natural language understanding tasks and has become an important reference for evaluating LLMs. Subsequently, benchmarks such as CMMLU~\citep{li2023cmmlu} and C-Eval~\citep{huang2023ceval} collect multi-disciplinary, multi-task questions from Chinese university exams, professional qualification tests, and textbooks, effectively assessing models’ general knowledge and their understanding. Beyond general capability evaluation, researchers have also developed Chinese benchmarks targeting specific advanced skills. For example, in mathematical reasoning, CMATH~\citep{wei2023cmath} tests models’ abilities to solve complex mathematical problems. Meanwhile, EduBench~\citep{xu2025edubench} constructs synthetic corpora for the education, but its question types are relatively limited, making it difficult to fully capture models’ Chinese potential. To address this critical gap, we propose OmniEduBench — a comprehensive Chinese education benchmark that uniquely combines knowledge and nurturing dimensions, providing a novel, holistic framework for systematically evaluating LLMs’ potential as educational assistants.